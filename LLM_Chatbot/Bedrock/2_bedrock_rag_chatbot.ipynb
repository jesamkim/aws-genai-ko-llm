{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df83f455-d264-4ec5-9145-e9369dd692cc",
   "metadata": {},
   "source": [
    "# Amazon Bedrock으로 RAG 적용된 Chatbot 만들기\n",
    "#### 목표 : 한글이 지원되는 Claude-v2 model로 RAG 기반 챗봇 만들기\n",
    "\n",
    "> *이 노트북은 SageMaker Studio 커널 **`Data Science 3.0`** 에서 잘 동작합니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd69693-c087-4cbd-8890-9e84da5686be",
   "metadata": {},
   "source": [
    "### Amazon Bedrock과 Langchain 기술을 활용하는 챗봇\n",
    "\n",
    "![chatbot_bedrock](./img/chatbot_bedrock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24105ca2-11b2-4230-a71e-3978aeef72c8",
   "metadata": {},
   "source": [
    "### Amazon Bedrock으로 챗봇을 구축하기 위한 랭체인 프레임워크\n",
    "- 챗봇과 같은 대화형 인터페이스에서는 단기적 수준뿐만 아니라 장기적 수준에서도 이전 상호 작용을 기억하는 것이 매우 중요합니다.\n",
    "- LangChain은 메모리 구성 요소를 두 가지 형태로 제공합니다.먼저 LangChain은 이전 채팅 메시지를 관리하고 조작하기 위한 도우미 유틸리티를 제공합니다.모듈식으로 설계되어 사용 방식에 관계없이 유용하게 사용할 수 있습니다.둘째, LangChain은 이러한 유틸리티를 체인에 통합하는 쉬운 방법을 제공합니다. 이를 통해 다양한 유형의 추상화를 쉽게 정의하고 상호 작용할 수 있으므로 강력한 챗봇을 쉽게 구축할 수 있습니다.\n",
    "- RAG(Retrief-Augmented Generation)를 통해 외부 데이터로 모델의 기초 데이터를 보완합니다. LangChain의 Conversational RetrieveChain 클래스를 사용하여 챗봇과 RAG 기능을 한 번의 통화로 결합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9ffdc-7957-45c9-887d-5fa3f775cb4e",
   "metadata": {},
   "source": [
    "### 컨텍스트를 활용한 챗봇 구축하기 - 핵심 요소\n",
    "\n",
    "컨텍스트 인식 챗봇을 구축하는 첫 번째 프로세스는 컨텍스트에 대한임베딩을 생성하는 것입니다.일반적으로 임베딩 모델을 통해 실행되고 일종의 벡터 저장소에 저장될 임베딩을 생성하는 통합 프로세스가 있습니다. 이 예제에서는 이를 위해 Amazon Titan 임베딩 모델을 사용하고 있습니다.\n",
    "\n",
    "![embeddings_lang](./img/embeddings_lang.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c17235-4d59-4975-bdcc-d94dfae92846",
   "metadata": {},
   "source": [
    "두 번째 프로세스는 사용자 요청 오케스트레이션, 상호 작용, 호출 및 결과 반환입니다.\n",
    "\n",
    "![chatbot_lang](./img/chatbot_lang.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a843bb5-aad9-4304-835c-9b673f9e5059",
   "metadata": {},
   "source": [
    "### 목표 아키텍처 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e54f37-f097-4cc1-bf1f-c385927ef558",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- 몇 가지 추가 종속성이 필요합니다.\n",
    "> FAISS : 벡터 임베딩 저장용 <br>\n",
    "> iPyWidgets : 노트북의 대화형 UI 위젯용 <br>\n",
    "> PyPDF : PDF 파일 처리용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d6ca89-0346-4612-9712-71cf05fa40e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \"faiss-cpu>=1.7,<2\" \"ipywidgets>=7,<8\" langchain==0.0.308 \"pypdf>=3.8,<4\" sqlalchemy==2.0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966de98-d709-4626-972f-2076ef2ec567",
   "metadata": {},
   "source": [
    "## 1. Amazon Bedrock Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48926f48-70d4-4d08-9ff5-f213c74a438b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a45661-2a40-4cec-8a1e-6a6b0e3a83b3",
   "metadata": {},
   "source": [
    "## 2. 챗봇 (기본 - 컨텍스트 없음)\n",
    "\n",
    "Anthropic의 Claude-v2 모델을 사용하여 한글이 지원되는 챗봇을 만들 것 입니다.\n",
    "\n",
    "우리는 LangChain의 [CoversationChain](https://python.langchain.com/en/latest/modules/models/llms/integrations/bedrock.html?highlight=ConversationChain#using-in-a-conversation-chain)을 사용하여 시작합니다. 또한 메시지 저장을 위해 [ConversationBufferMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer.html)를 사용합니다. 메시지 목록으로 기록을 얻을 수도 있습니다(채팅 모델에서 매우 유용합니다).\n",
    "\n",
    "챗봇은 이전 상호작용을 기억해야 합니다. 대화 기억을 통해 우리는 그렇게 할 수 있습니다. 대화형 메모리를 구현하는 방법에는 여러 가지가 있습니다. LangChain의 맥락에서 이들은 모두 ConversationChain 위에 구축됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d7503af-4ad1-4abe-9418-d8dfc56e3d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 안녕하세요?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 네, 안녕하세요!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "cl_llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\"max_tokens_to_sample\": 1024,\n",
    "                  \"temperature\": 0.1,\n",
    "                  \"top_k\": 250,\n",
    "                  \"top_p\": 1.0\n",
    "                 },\n",
    ")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=cl_llm, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "print_ww(conversation.predict(input=\"안녕하세요?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6086e2d-fd79-4c70-8c1b-3f015aae9264",
   "metadata": {},
   "source": [
    "### 결과 분석\n",
    "[효과적인 Claude 프롬프트](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design)는  `\\n\\nHuman: 텍스트 \\n\\nAassistant:` 형식이어야 합니다.\n",
    "\n",
    "Claude의 프롬프트 작성 방법에 대해 자세히 알아보려면 [Anthropic 문서](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design)를 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93720ef9-3f64-42d6-9004-1e7a6bf7ca55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain version check: 0.0.308\n",
      "boto3 version check: 1.28.60\n"
     ]
    }
   ],
   "source": [
    "# boto3 및 langchain 버전 확인\n",
    "# 2023년 10월 5일 기준 langchain 버전은 0.0.304 입니다.\n",
    "\n",
    "import langchain\n",
    "\n",
    "print(f\"langchain version check: {langchain.__version__}\")\n",
    "print(f\"boto3 version check: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540462e1-b8b2-4462-bcba-bbd784fc4084",
   "metadata": {},
   "source": [
    "## 3. 프롬프트 템플릿(Langchain)을 이용한 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490dec67-541b-473c-a917-f0578a1b45bf",
   "metadata": {},
   "source": [
    "LangChain은 프롬프트를 쉽게 구성하고 작업할 수 있도록 여러 클래스와 기능을 제공합니다.\n",
    "\n",
    "[PromptTemplate](https://python.langchain.com/en/latest/modules/prompts/getting_started.html) 클래스를 사용하여 f-string 템플릿에서 프롬프트를 구성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1f61d81-3565-43dd-a19b-de6f0fbb1b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude_prompt: \n",
      " input_variables=['history', 'input'] template='다음은 사람과 인공지능의 친근한 대화입니다. 인공지능은 말이 많고 상황에 맞는 구체적인 세부 정보를 많이 제공합니다. 인공지능은 모른다면 질문에 대한 대답은 솔직히 모른다고 말합니다.\\n\\nCurrent conversation:\\n{history}\\n\\nHuman: {input}\\n\\nAssistant:\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# turn verbose to true to see the full logs and documents\n",
    "conversation= ConversationChain(\n",
    "    llm=cl_llm, verbose=False, memory=ConversationBufferMemory() #memory_chain\n",
    ")\n",
    "\n",
    "# langchain prompts do not always work with all the models. This prompt is tuned for Claude\n",
    "\n",
    "claude_prompt = PromptTemplate.from_template(\"\"\"다음은 사람과 인공지능의 친근한 대화입니다. 인공지능은 말이 많고 상황에 맞는 구체적인 세부 정보를 많이 제공합니다. 인공지능은 모른다면 질문에 대한 대답은 솔직히 모른다고 말합니다.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "\n",
    "Assistant:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"claude_prompt: \\n\", claude_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a899dba-edd4-47c7-95be-dc368f39dd71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat_memory = ConversationBufferMemory(human_prefix='Human', ai_prefix='Assistant')\n",
    "conversation = ConversationChain(llm=cl_llm, verbose=False, memory=chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842e245f-9592-46b0-9a8f-e192121f4d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 안녕하세요! 반갑습니다.\n"
     ]
    }
   ],
   "source": [
    "conversation.prompt = claude_prompt\n",
    "\n",
    "print_ww(conversation.predict(input=\"안녕하세요?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7ae81-4626-402e-a6f7-4547ffcf7005",
   "metadata": {},
   "source": [
    "#### (1) 새로운 질문\n",
    "\n",
    "모델이 초기 메시지로 응답했습니다. 몇 가지 질문을 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0367ea49-f9f2-4dda-80b4-4bff62fe0777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 네, 새로운 정원을 시작하는 몇 가지 팁을 알려드리겠습니다.\n",
      "\n",
      "먼저, 정원의 위치를 잘 선택하는 것이 중요합니다. 해가 잘 드는 곳으로 선택하시는 것이 좋습니다. 또한 물 공급이 용이한 곳을 고르세요.\n",
      "\n",
      "다음으로 흙의 상태를 확인하세요. 흙이 너무 건조하거나 점토질이면 퇴비를 추가로 넣어주는 것이 좋습니다. pH 수준도 확인하세요.\n",
      "\n",
      "정원 배치를 계획할 때에는 저온성 작물과 고온성 작물을 구분하여 심는 것이 중요합니다. 또한 키가 큰 식물과 작은 식물의 배치도 중요합니다.\n",
      "\n",
      "물주기와 관수 방법도 식물에 따라 다르므로 잘 확인하세요. 일정한 시기에 관수와 배수를 해주는 것이 식물 성장에 도움이 됩니다.\n",
      "\n",
      "마지막으로, 해충 방지와 병충해 관리도 철저히 하는 것이 좋습니다. 이를 통해 건강한 정원을 가꿀 수 있습니다.\n",
      "\n",
      "정원 가꾸기에 관심이 많으시면 책이나 온라인을 통해 더 많은 정보를 찾아보시기 바랍니다. 즐거운 정원 가꾸기 되세요!\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"새로운 정원을 시작하는 방법에 대한 몇 가지 팁을 알려주세요.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19810dd7-0402-4164-8554-9c77ba503b93",
   "metadata": {},
   "source": [
    "#### (2) 질문을 토대로 작성\n",
    "\n",
    "모델이 이전 대화를 이해할 수 있는지 확인하기 위해 정원이라는 단어를 언급하지 않고 질문해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2a34b3-3c05-4f98-a763-b271ffc4b40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 네, 제가 말씀드린 팁들은 토마토 정원을 가꾸는 데도 적용할 수 있습니다.\n",
      "\n",
      "토마토는 일교차가 크고 햇볕이 잘 드는 곳을 좋아하므로 그런 곳을 정원 위치로 선택하세요. 또한 토마토는 물이 잘 빠지는 토양을 선호하므로 배수가 잘 되는 곳이 좋습니다.\n",
      "\n",
      "토마토를 심기 전에 흙의 pH를 확인하고 필요하다면 석회나 퇴비를 추가로 넣어주세요. 토마토는 약산성 토양을 선호합니다.\n",
      "\n",
      "토마토는 성장 형태에 따라 결실기가 짧은 종류와 긴 종류가 있는데, 이를 고려하여 정원 내 배치를 계획하세요.\n",
      "\n",
      "물주기는 토마토 열매가 익기 시작하면 조금 줄이고, 열매가 익으면 물을 주지 않는 것이 좋습니다.\n",
      "\n",
      "이외에도 병해충 방지, 지주 설치, 시비 등을 철저히 하시면 토마토 정원 가꾸기에 도움이 될 것입니다.\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"좋아요. 토마토에도 어울릴까요?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c79fc-9781-4256-a9d1-107366cc80ae",
   "metadata": {},
   "source": [
    "#### (3) 대화 마무리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfdf3fca-b7e5-4fa7-bfc2-6f89dc558f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 네, 처음 새로운 정원을 시작하시는 것이다보니 정보가 많이 필요하실 것 같습니다. 제가 알려드린 내용이 새로운 정원 가꾸기에 도움이 되었기를 바랍니다. 더 궁금한 점이 있으시면\n",
      "언제든지 문의주세요. 즐거운 정원 가꾸기 되세요!\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"그게 다야, 고마워!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eaec32-ff9f-41a2-a639-6409555d2a06",
   "metadata": {},
   "source": [
    "## 4. ipywidgets를 사용한 대화형 세션\n",
    "\n",
    "다음 유틸리티 클래스를 사용하면 Claude와 보다 자연스러운 방식으로 상호 작용할 수 있습니다. 입력창에 질문을 적고 Claude의 답변을 받습니다. 그렇게 대화를 이어갈 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4472a34c-e126-4e61-8d31-f4759d1308fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        result = self.qa.run({'question': prompt })\n",
    "                    else:\n",
    "                        result = self.qa.run({'input': prompt }) #, 'history':chat_history})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print_ww(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ece29-9271-4f53-aa19-90581cdd1b75",
   "metadata": {},
   "source": [
    "#### (1) 채팅을 시작해 보겠습니다. 다음 질문을 테스트할 수도 있습니다.\n",
    "\n",
    "1. 농담 하나 해줘\n",
    "2. 또 다른 농담을 들려주세요\n",
    "3. 첫 번째 농담은 무엇이었나요?\n",
    "4. 첫 번째 농담과 같은 주제로 또 다른 농담을 할 수 있나요?\n",
    "\n",
    "위의 4가지를 순서대로 아래에 입력하시고, \"Send\" 버튼을 눌러 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c685fccc-fc06-46b8-a991-34de9fc7635f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bdafe87aef4d24bf651f6ef171e016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(conversation)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec026d6-4268-4931-8a06-4a0d65687229",
   "metadata": {},
   "source": [
    "## 5. 페르소나를 활용한 챗봇\n",
    "- skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73454db9-6835-4d49-8b18-29431f913bb3",
   "metadata": {},
   "source": [
    "## 6. 맥락을 가진 챗봇 (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f2b58-7078-4e3d-b583-2b8659b0bde7",
   "metadata": {},
   "source": [
    "이 사용 사례에서는 Chatbot에게 이전에 본 적이 없는 외부 코퍼스의 질문에 답변하도록 요청합니다. 이를 위해 RAG(Retrieval Augmented Generation)라는 패턴을 적용합니다. 아이디어는 말뭉치를 덩어리로 인덱싱한 다음 덩어리와 질문 사이의 의미론적 유사성을 사용하여 말뭉치의 어느 섹션이 답변을 제공하는 데 관련될 수 있는지 찾는 것입니다. 마지막으로 가장 관련성이 높은 청크가 집계되어 기록을 제공하는 것과 유사하게 ConversationChain에 컨텍스트로 전달됩니다.\n",
    "\n",
    "<b>Titan Embeddings Model</b>을 사용하여 벡터를 생성하겠습니다. 그런 다음 이 벡터는 메모리 내 벡터 데이터 저장소를 제공하는 오픈 소스 라이브러리인 FAISS에 저장됩니다. 챗봇이 질문을 받으면 FAISS에 질문을 쿼리하고 의미상 가장 가까운 텍스트를 검색합니다. 이것이 우리의 대답이 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a0c1a-0835-4e16-92c2-77cea7deedb2",
   "metadata": {},
   "source": [
    "### 6.1 2022년 아마존 주주 서한 pdf 문서로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a3f6f-fdd2-4464-8477-51e360c7fb1e",
   "metadata": {},
   "source": [
    "#### Amazon Titan Embedding 모델 사용\n",
    "\n",
    "- model_id='amazon.titan-embed-text-v1'\n",
    "- 이 모델은 최대 512 Token 입력이 가능합니다. 추후에 최대 토큰 사이즈가 큰 모델이 나오면 바꾸어서 테스트하시기 바랍니니다.\n",
    "- 일반적으로 한글 임베딩시에 512 토큰은 작습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0c3ad31-c6bc-430f-9cd5-45941b8f8d70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BedrockEmbeddings(client=<botocore.client.BedrockRuntime object at 0x7f512bd65300>, region_name=None, credentials_profile_name=None, model_id='amazon.titan-embed-text-v1', model_kwargs=None, endpoint_url=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "bedrock_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31872d71-7672-44f6-b7a8-a7652df86b1e",
   "metadata": {},
   "source": [
    "#### PyPDFDirectoryLoader 를 통한 PDF 파일 로딩\n",
    "\n",
    "- chunk_size 는 임베딩 모델의 최대 입력 토큰이 512를 고려 해서 정했습니다.\n",
    "- 아래 수치보다 증가시킬 경우에 Embedding Model 에 벡터 변환을 요구할 시에, 토큰이 512 보다 커서 에러가 발생합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0949aaa9-806f-45d9-b623-3d9249222ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from langchain.indexes.vectorstore import VectorStoreIndexWrapper 에서 에러가 나면 sqlalchemy를 최신버전으로 업데이트 합니다.\n",
    "#!pip install sqlalchemy==2.0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91428474-e7d2-4f6b-9701-73e72077e17e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.21\n"
     ]
    }
   ],
   "source": [
    "# 테스트 기준 sqlalchemy==2.0.21\n",
    "\n",
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bbff621-8333-48c9-ae9d-cb05e6daf52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"rag_data_kr_pdf/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 230,\n",
    "    chunk_overlap  = 50,\n",
    "#    separators = ['\\n']\n",
    "#    separators = ['\\n','\\n\\n']    \n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e741be37-f249-4c6b-a454-e314fa2aae69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore_faiss_aws: number of elements in the index=92::\n",
      "CPU times: user 306 ms, sys: 22.6 ms, total: 329 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorstore_faiss_aws = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "    embedding = bedrock_embeddings\n",
    ")\n",
    "\n",
    "print(f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\")\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7281fa00-9af0-43fe-8c7f-0b82ff8ecf82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 10 documents loaded is 1606 characters.\n",
      "After the split we have 92 documents more than the original 10.\n",
      "Average length among 92 documents (after split) is 216 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8568e8fe-54e4-45e4-9b5c-17a5a1012534",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs[0].page_content: \n",
      " CEO로서 두 번째 연례 주주 서한을 작성하기 위해 자리에 앉았을 때 저는 Amazon의 앞날에 대해 낙관적이고 활력을 얻었습니다. 2022년은 최근 기억에 있어 어려운 거시경제적 해 중 하나이고 우리 자체의 운영상의 어려움에도 불구하고 우리는 여전히 수요를 늘릴 방법을 찾았습니다(팬데믹 전반기에 경험한 전례 없는 성장에 더해). 우리는 장단기적으로 고객 경험을 의미 있게 개선하기 위해 대규모 사업을\n"
     ]
    }
   ],
   "source": [
    "print(\"docs[0].page_content: \\n\", docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e72b6272-32f9-4215-a451-17940c2a3d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding of a document chunk:  [ 0.68359375 -0.359375    0.27148438 ...  0.203125   -0.56640625\n",
      " -0.53125   ]\n",
      "Size of the embedding:  (1536,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef827fd-30a2-4253-9bb6-0e4397bb4934",
   "metadata": {},
   "source": [
    "#### 시맨틱 검색 (Semantic search)\n",
    "\n",
    "LangChain에서 제공하는 Wrapper 클래스를 사용하여 벡터 데이터베이스 저장소를 쿼리하고 관련 문서를 반환할 수 있습니다. 뒤에서는 RetrievalQA 체인만 실행됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ecf47b7-76ec-41bd-8977-d7d0b78e4b83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 제가 이 편지의 전체 내용을 보지 않았기 때문에 아마존의 Generative AI 전략에 대해 확실히 말씀드리기 어렵습니다. 편지에서 Generative AI나 LLM에 대한\n",
      "구체적인 전략이 언급된 부분이 없는 것 같습니다. 저는 이 질문에 대한 정확한 답변을 제공할 수 있는 충분한 정보가 없습니다. 죄송합니다. 아마존의 Generative AI 전략에\n",
      "대한 더 많은 정보가 있다면, 그것을 바탕으로 답변을 제공할 수 있을 것 같습니다.\n"
     ]
    }
   ],
   "source": [
    "query = \"아마존은 Generative AI 의 전략이 무엇인가요?\"\n",
    "print_ww(wrapper_store_faiss.query(query, llm=cl_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b2573-ffa1-4114-a4f6-89d5c3aa5072",
   "metadata": {},
   "source": [
    "-----\n",
    "의미론적 검색이 어떻게 작동하는지 살펴보겠습니다.\n",
    "\n",
    "먼저 쿼리에 대한 임베딩 벡터를 계산하고\n",
    "그런 다음 이 벡터를 사용하여 벡터 스토어에서 유사성 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb56ff7a-2639-4691-8640-13f0fbb373aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.765625, -0.59375, -0.08300781, -0.29296875, 0.86328125, -0.19140625, 0.06542969, -0.00034332275, -1.2109375, -0.578125]\n",
      "국제적으로 확장하고, 아직 아마존의 초기 단계인 대규모 소매 시장 부문을 추구하고, 판매자가 자신의 웹사이트에서 보다 효과적으로 판매할 수 있도록 돕기 위해 우리의 고유한 자산을\n",
      "사용하는 것은 우리에게 어느 정도 자연스러운 확장입니다. 핵심 비즈니스에서 더 멀리 떨어져 있지만 고유한 기회가 있는 투자도 몇 가지 있습니다. 2003년에는 AWS가 전형적인\n",
      "예였습니다. 2023년에는 Amazon\n",
      "----\n",
      "LLM과 GeneraTve AI가 그렇게 변혁적일 것이라고 생각하기 때문에 전체 편지를 쓸 수 있지만 향후 편지로 남겨두겠습니다. LLM과 GeneraTve AI가 고객, 주주 및\n",
      "Amazon에게 큰 문제가 될 것이라고 가정해 보겠습니다.  그래서 마지막으로 저는 우리가 이 도전적인 거시경제 시대에 진입했을 때보다 더 강력한 위치에서 등장할 것이라고\n",
      "낙관합니다. 여기에는 몇 가지 이유가 있으며 위에서 많은\n",
      "----\n",
      "잠재력이 있으며 기술 및 창의적 능력을 갖춘 회사가 상대적으로 적다는 점에서 AWS와 유사합니다. 그것을 추구하는 투자 가설로.  제가 언급할 마지막 투자 영역은 Amazon이\n",
      "앞으로 수십 년 동안 모든 비즈니스 영역에서 발명할 수 있도록 설정하는 핵심이며 대규모 언어 모델(\"LLM\") 및 생성 AI입니다. 머신 러닝은 수십 년 동안 유망한 기술이었지만\n",
      "기업에서 널리 사용되기 시작한 것은 불과 5~10년\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "v = bedrock_embeddings.embed_query(query)\n",
    "print(v[0:10])\n",
    "results = vectorstore_faiss_aws.similarity_search_by_vector(v, k=3)\n",
    "for r in results:\n",
    "    print_ww(r.page_content)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da90e0-982d-46d0-a317-cec43c7e3fe8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 메모리\n",
    "\n",
    "모든 챗봇에는 사용 사례에 따라 맞춤화된 다양한 옵션을 갖춘 QA 체인이 필요합니다. 그러나 챗봇에서는 모델이 답변을 제공하기 위해 이를 고려할 수 있도록 항상 대화 기록을 보관해야 합니다. 이 예에서는 대화 기록을 유지하기 위해 ConversationBufferMemory와 함께 LangChain의 [ConversationalRetrievalChain](https://python.langchain.com/docs/modules/chains/popular/chat_Vector_db)을 사용합니다.\n",
    "\n",
    "출처: https://python.langchain.com/docs/modules/chains/popular/chat_Vector_db\n",
    "\n",
    "뒤에서 무슨 일이 일어나고 있는지 모두 보려면 'verbose'를 'True'로 설정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f57c92bd-0ac1-41f6-8944-ab873c0fc51e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following conversation and a follow up question, rephrase the follow up question to be a\n",
      "standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "{chat_history}\n",
      "Follow Up Input: {question}\n",
      "Standalone question:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "print_ww(CONDENSE_QUESTION_PROMPT.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c3113-2f40-4dd6-92a2-44bbe1b2ba50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ConversationRetrievalChain에 사용되는 매개변수\n",
    "\n",
    "* **retriever**: 우리는 `VectorStore`가 지원하는 `VectorStoreRetriever`를 사용했습니다. 텍스트를 검색하려면 `\"similarity\"` 또는 `\"mmr\"`라는 두 가지 검색 유형을 선택할 수 있습니다. `search_type=\"similarity\"`는 질문 벡터와 가장 유사한 텍스트 청크 벡터를 선택하는 검색 객체에서 유사성 검색을 사용합니다.\n",
    "\n",
    "* **메모리**: 이력을 저장하는 메모리 체인\n",
    "\n",
    "* **dense_question_prompt**: 사용자의 질문이 주어지면 이전 대화와 해당 질문을 사용하여 독립형 질문을 구성합니다.\n",
    "\n",
    "* **chain_type**: 채팅 기록이 길고 상황에 맞지 않는 경우 이 매개변수를 사용하고 옵션은 `stuff`, `refine`, `map_reduce`, `map-rerank`입니다.\n",
    "\n",
    "질문이 컨텍스트 범위를 벗어나면 모델은 답을 모른다고 대답합니다.\n",
    "\n",
    "**참고**: 체인이 어떻게 작동하는지 궁금하다면 `verbose=True` 줄의 주석 처리를 해제하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd77c6db-d4c0-4967-98cb-aa1c4e7ce56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn verbose to true to see the full logs and documents\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cl_llm, \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    memory=memory_chain,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    #verbose=True, \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2537e41-eb23-46ed-9af5-76e84c296872",
   "metadata": {},
   "source": [
    "채팅을 해보죠. 아래와 같은 질문을 해보세요. \n",
    "1. 아마존은 Generative AI 의 전략이 무엇인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "683edf42-9dcb-4406-9e35-9b7e1a6b6167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5145cce75bdf4d7488696913bacdf7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(qa, retrievalChain=True)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91035292-c592-487a-b705-87665c8ac3a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "이는 이 노트북의 시작 부분에 설명된 것과 동일한 이유로 발생합니다. 기본 랭체인 프롬프트는 Claude에게 최적이 아닙니다. 다음 셀에서는 두 개의 새로운 프롬프트를 설정하겠습니다. 하나는 질문의 표현을 변경하기 위한 것이고, 다른 하나는 해당 질문의 답변을 얻기 위한 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f1aaf5b-0d74-4f2b-89d7-87fbc09b714b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn verbose to true to see the full logs and documents\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "\n",
    "# We are also providing a different chat history retriever which outputs the history as a Claude chat (ie including the \\n\\n)\n",
    "_ROLE_MAP = {\"human\": \"\\n\\nHuman: \", \"ai\": \"\\n\\nAssistant: \"}\n",
    "def _get_chat_history(chat_history):\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        if isinstance(dialogue_turn, BaseMessage):\n",
    "            role_prefix = _ROLE_MAP.get(dialogue_turn.type, f\"{dialogue_turn.type}: \")\n",
    "            buffer += f\"\\n{role_prefix}{dialogue_turn.content}\"\n",
    "        elif isinstance(dialogue_turn, tuple):\n",
    "            human = \"\\n\\nHuman: \" + dialogue_turn[0]\n",
    "            ai = \"\\n\\nAssistant: \" + dialogue_turn[1]\n",
    "            buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported chat history format: {type(dialogue_turn)}.\"\n",
    "                f\" Full chat history: {chat_history} \"\n",
    "            )\n",
    "    return buffer\n",
    "\n",
    "# the condense prompt for Claude\n",
    "condense_prompt_claude = PromptTemplate.from_template(\"\"\"{chat_history}\n",
    "\n",
    "새로운 질문으로만 대답하세요.\n",
    "\n",
    "\n",
    "Human: 이전 대화를 고려하여 질문을 어떻게 하시겠습니까?: {question}\n",
    "\n",
    "\n",
    "Assistant: Question:\"\"\")\n",
    "\n",
    "# recreate the Claude LLM with more tokens to sample - this provide longer responses but introduces some latency\n",
    "cl_llm = Bedrock(model_id=\"anthropic.claude-v1\", client=boto3_bedrock, model_kwargs={\"max_tokens_to_sample\": 500})\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cl_llm, \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    #retriever=vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 8}),\n",
    "    memory=memory_chain,\n",
    "    get_chat_history=_get_chat_history,\n",
    "    verbose=True,\n",
    "    condense_question_prompt=condense_prompt_claude, \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")\n",
    "\n",
    "# the LLMChain prompt to get the answer. the ConversationalRetrievalChange does not expose this parameter in the constructor\n",
    "qa.combine_docs_chain.llm_chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "{context}\n",
    "\n",
    "\n",
    "Human: <q></q> XML 태그 내의 질문에 답하려면 최대 3개의 문장을 사용하세요.\n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "답변에 XML 태그를 사용하지 마세요. 답변이 context에 없으면 \"죄송합니다. 문맥에서 답을 찾을 수 없어서 모르겠습니다.\"라고 말합니다.\n",
    "\n",
    "Assistant:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88381dec-480a-4e74-b51c-bcc199129b68",
   "metadata": {},
   "source": [
    "다시 채팅을 해보죠. 아래와 같은 질문을 해보세요.\n",
    "\n",
    "1. 아마존은 Generative AI 의 전략이 무엇인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "748654bb-30c3-4d9b-a1f2-cfc3a9308cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568b615bc1ee4dc5a7dd07a0113f543a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(qa, retrievalChain=True)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04c00e-adf0-4bb5-97db-96b44e80e0da",
   "metadata": {},
   "source": [
    "\"아마존은 Generative AI 의 전략이 무엇인가요?\" 라는 질문에 주주 서한 문서 기반으로 답변을 출력하는 것을 확인할 수 있습니다.\n",
    "\n",
    "![rag-chatbot-result-01](./img/rag-chatbot-result-01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424cdce-8a73-4675-976b-f87926bbb4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3bd46-0033-4fdd-b338-27ea9bd16359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
